{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "460e4ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total RAM: 201.34 GB\n",
      "Available RAM: 188.80 GB\n",
      "Used RAM: 10.70 GB\n",
      "RAM Usage: 6.2%\n",
      "Wed Jul 23 10:53:48 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A40          Off  | 00000000:3B:00.0 Off |                    0 |\n",
      "|  0%   27C    P8    12W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A40          Off  | 00000000:AF:00.0 Off |                    0 |\n",
      "|  0%   26C    P8    12W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A40          Off  | 00000000:D8:00.0 Off |                    0 |\n",
      "|  0%   28C    P8    13W / 300W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "# Get total, used, and available memory in GB\n",
    "mem = psutil.virtual_memory()\n",
    "print(f\"Total RAM: {mem.total / 1e9:.2f} GB\")\n",
    "print(f\"Available RAM: {mem.available / 1e9:.2f} GB\")\n",
    "print(f\"Used RAM: {mem.used / 1e9:.2f} GB\")\n",
    "print(f\"RAM Usage: {mem.percent}%\")\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11171515",
   "metadata": {
    "libroFormatter": "formatter-string",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find 3 GPU can be used.\n",
      "GPU 1: NVIDIA A40\n",
      "GPU 2: NVIDIA A40\n",
      "GPU 3: NVIDIA A40\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 检查CUDA是否可用\n",
    "if torch.cuda.is_available():\n",
    "    gpu_count = torch.cuda.device_count()\n",
    "    print(f\"Find {gpu_count} GPU can be used.\")\n",
    "\n",
    "    for i in range(gpu_count):\n",
    "        gpu_name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i + 1}: {gpu_name}\")\n",
    "else:\n",
    "    print(\"No GPU can be used.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0949d68",
   "metadata": {
    "libroFormatter": "formatter-string",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "try:\n",
    "    from rouge import Rouge\n",
    "except:\n",
    "    !pip install rouge\n",
    "    from rouge import Rouge\n",
    "# from bert_score import score\n",
    "import statistics\n",
    "from ast import literal_eval\n",
    "import functools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "random.seed(8888)\n",
    "torch.manual_seed(8888)\n",
    "random.seed(8888)\n",
    "np.random.seed(8888)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(8888)\n",
    "    torch.cuda.manual_seed_all(8888)\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a336ef85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.11.13 (main, Jun  5 2025, 13:12:00) [GCC 11.2.0]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9adc3d49",
   "metadata": {
    "libroFormatter": "formatter-string",
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import json\n",
    "from os.path import join\n",
    "\n",
    "model_dir = \"../models\"\n",
    "# model_name = 'Meta-Llama-3-2B'  #gemma-2-9b-it  gemma-2-9b\n",
    "# model_name = 'Llama-3.2-1B'\n",
    "model_name = 'DeepSeek-R1-Distill-Qwen-1.5B'\n",
    "output_dir = '../outputs'\n",
    "dataset_dir = '../dataset'\n",
    "\n",
    "#'Meta-Llama-3-8B-Instruct' #'Llama-2-7b-chat-hf' #'llama-7b' \n",
    "# \"OLMo-7B-Instruct\" olmo-2-1124-7B-Instruct 'OLMo-2-1124-7B'  \"OLMo-7B-0724-Instruct-hf\" olmo1的情况都非常反常，不知道为什么\n",
    "# Qwen-7B, Qwen1.5-7B, Qwen-7B-Chat, Qwen1.5-7B-Chat, Qwen2-7B-Instruct, Qwen2.5-7B-Instruct, Qwen2.5-Coder-7B, Qwen2.5对中文做过优化，情况不太一样\n",
    "# Mistral-7B-Instruct-v0.1, Mistral-7B-Instruct-v0.2, Mistral-7B-Instruct-v0.3, Mistral-7B-v0.3\n",
    "# Yi-1.5-6B-Chat, Yi-6B-Chat\n",
    "# gemma-2-9b-it gemma-7b-it, gemma-2-9b\n",
    "# gpt-j-6b, mpt-7b-chat, opt-6.7b, pythia-6.9b, zephyr-7b-beta, falcon-7b-instruct\n",
    "# deepseek\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    join(model_dir, model_name),\n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(join(model_dir, model_name), trust_remote_code=True)\n",
    "\n",
    "if 'llama' in model.config.model_type.lower() or 'mistral' in model.config.model_type.lower() or 'yi' in model.config.model_type.lower() or 'gptj' in model.config.model_type.lower():\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "# elif 'qwen' in model.config.model_type.lower(): # not sure if this needs to be modified for Deepseek-R1\n",
    "    tokenizer.pad_token = '<|endoftext|>'\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    # in gemma, pad_token_id = 0 is default\n",
    "    # in olmo, pad_token_id = 1 is default\n",
    "    \n",
    "    \n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "model.to('cuda');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fe3fc3a",
   "metadata": {
    "libroFormatter": "formatter-string",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 1536)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "          (k_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (v_proj): Linear(in_features=1536, out_features=256, bias=True)\n",
      "          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      ")\n",
      "model.config:  Qwen2Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151643,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 1536,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8960,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"max_window_layers\": 21,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.53.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_mrope\": false,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "model.config.model_type.lower():  qwen2\n"
     ]
    }
   ],
   "source": [
    "print('model: ',model)\n",
    "print('model.config: ',model.config)\n",
    "print('model.config.model_type.lower(): ',model.config.model_type.lower())  # Often provides a string identifier\n",
    "# print('model.config.num_hidden_layers: ',model.config.num_hidden_layers)\n",
    "\n",
    "\n",
    "if 'gptj' in model.config.model_type.lower():\n",
    "    model_layers_num = int(model.config.n_layer)  \n",
    "    mlp_vector_num = 16384\n",
    "    mlp_dim_num = int(model.config.n_embd)\n",
    "    layer_name = 'transformer.h'\n",
    "    mlp_name = 'mlp'\n",
    "    mlp_last_layer_name = 'fc_out'\n",
    "\n",
    "elif 'qwen' in model.config.model_type.lower() and 'qwen2' not in model.config.model_type.lower(): #qwen1, qwen2.5\n",
    "    layer_name = 'transformer.h'\n",
    "    mlp_name = 'mlp'\n",
    "    mlp_last_layer_name = 'w2'\n",
    "    mlp_dim_num = int(model.config.hidden_size)\n",
    "    model_layers_num = int(model.config.num_hidden_layers)\n",
    "    mlp_vector_num = int(model.config.intermediate_size / 2)\n",
    "    \n",
    "else:\n",
    "    model_layers_num = int(model.config.num_hidden_layers)  # on olmo1, olmo2, qwen2, qwen2.5, llama, ...\n",
    "    mlp_vector_num = int(model.config.intermediate_size)\n",
    "    mlp_dim_num = int(model.config.hidden_size)\n",
    "    layer_name = 'model.layers' \n",
    "    mlp_name = 'mlp'\n",
    "    mlp_last_layer_name = 'down_proj'\n",
    "    attn_name = 'self_attn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2c7476f",
   "metadata": {
    "libroFormatter": "formatter-string",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Start Answering -------------------\n",
      "layers_to_cache:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
      "----------------- Running no cot Inference -------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [00:20, 29.98it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "n_new_tokens = 100\n",
    "NUll_num = 0\n",
    "\n",
    "def form_options(options: list):\n",
    "    option_str = 'Options are:\\n'\n",
    "    opts = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J']\n",
    "    for opt, o in zip(options, opts):\n",
    "        option_str += f'({o}): {opt}' + '\\n'\n",
    "    return option_str\n",
    "\n",
    "\n",
    "def get_prediction(output):\n",
    "    pattern = r\"answer is \\(?([ABCDEFGHIJ])\\)?\"\n",
    "    match = re.search(pattern, output)\n",
    "    if match:\n",
    "        #print('prediction success: ',match.group(1))\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        #print(\"extraction failed, do a random guess\")\n",
    "        global NUll_num  \n",
    "        NUll_num += 1\n",
    "        return random.choice(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J'])\n",
    "\n",
    "\n",
    "def generate_outputs(questions):\n",
    "    \n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=\"longest\", return_token_type_ids=False).to('cuda')\n",
    "    input_length = inputs.input_ids.size(1)\n",
    "    output = model(**inputs, output_hidden_states = True)\n",
    "#     Question_input = [[{\"role\": \"user\", \"content\": prompt}] for prompt in questions]\n",
    "#     texts = tokenizer.apply_chat_template(Question_input ,tokenize=False)\n",
    "\n",
    "#     inputs = tokenizer(texts, padding=\"longest\", return_tensors=\"pt\")\n",
    "#     inputs = {key: val.cuda() for key, val in inputs.items()}\n",
    "#     output = model(**inputs, output_hidden_states = True)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def generate_questions(questions):\n",
    "    \n",
    "    inputs = tokenizer(questions, return_tensors=\"pt\", padding=\"longest\", return_token_type_ids=False).to('cuda')\n",
    "    input_length = inputs.input_ids.size(1)\n",
    "    gen_tokens = model.generate(**inputs, max_new_tokens=n_new_tokens, do_sample=False)\n",
    "\n",
    "    gen_text = tokenizer.batch_decode(gen_tokens[:, input_length:], skip_special_tokens=True)\n",
    "    \n",
    "    return gen_text\n",
    "\n",
    "\n",
    "dataset = load_from_disk('../dataset/mmlu-pro')\n",
    "\n",
    "categories = ['computer science', 'math', 'chemistry', 'engineering', 'law', 'biology',\n",
    "              'health', 'physics', 'business', 'philosophy', 'economics', 'other',\n",
    "              'psychology', 'history']\n",
    "\n",
    "per_category_accuracy = {c: [0, 0] for c in categories}\n",
    "success, fail = 0, 0\n",
    "answers = []\n",
    "\n",
    "print('----------------- Start Answering -------------------')\n",
    "queries_batch = []  # 可以测试一下batch or single哪种方式准确率更高，更合适一些 #发现基本是一样的，padding不会对准确率造成影响\n",
    "entry_batch = []\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "random.seed(8888)\n",
    "\n",
    "test_data = list(dataset['test'])\n",
    "\n",
    "# example: running on 600 samples\n",
    "sampled_data_600 = random.sample(test_data, 600)\n",
    "\n",
    "layers_to_cache = list(range(model_layers_num))\n",
    "print('layers_to_cache: ',layers_to_cache)\n",
    "hs_cache_cot = {}\n",
    "hs_cache_no_cot = {}\n",
    "\n",
    "print('----------------- Running no cot Inference -------------------')\n",
    "for ix, entry in tqdm(enumerate(sampled_data_600)):\n",
    "        \n",
    "    query = 'Q: ' + entry['question'] + \"\\nA: \"\n",
    "    \n",
    "    queries_batch.append(query)\n",
    "    \n",
    "    if len(queries_batch) == batch_size or ix == len(dataset['test']) - 1:\n",
    "        output = generate_outputs(queries_batch)\n",
    "        \n",
    "        for layer in layers_to_cache:\n",
    "            if layer not in hs_cache_no_cot:\n",
    "                hs_cache_no_cot[layer] = output[\"hidden_states\"][layer][: ,-1 , :].cpu() #bs * tok * dims\n",
    "            else:\n",
    "                hs_cache_no_cot[layer] = torch.cat((hs_cache_no_cot[layer], output[\"hidden_states\"][layer][: ,-1 , :].cpu()), dim=0)\n",
    "\n",
    "        \n",
    "        queries_batch = []\n",
    "    torch.cuda.empty_cache()\n",
    "# # TODO print('----------------- Running cot Inference -------------------')\n",
    "\n",
    "sampled_data_3000 = random.sample(test_data, 3000)\n",
    "\n",
    "layers_to_cache = list(range(model_layers_num))\n",
    "print('layers_to_cache: ',layers_to_cache)\n",
    "# hs_cache_cot = {}\n",
    "hs_cache_no_cot_3000 = {}\n",
    "\n",
    "print('----------------- Running no cot Inference (3000 samples MMLU-pro) -------------------')\n",
    "for ix, entry in tqdm(enumerate(sampled_data_3000)):\n",
    "        \n",
    "    query = 'Q: ' + entry['question'] + \"\\nA: \"\n",
    "    \n",
    "    queries_batch.append(query)\n",
    "    \n",
    "    if len(queries_batch) == batch_size or ix == len(dataset['test']) - 1:\n",
    "        output = generate_outputs(queries_batch)\n",
    "        \n",
    "        for layer in layers_to_cache:\n",
    "            if layer not in hs_cache_no_cot_3000:\n",
    "                hs_cache_no_cot_3000[layer] = output[\"hidden_states\"][layer][: ,-1 , :].cpu() #bs * tok * dims\n",
    "            else:\n",
    "                hs_cache_no_cot_3000[layer] = torch.cat((hs_cache_no_cot_3000[layer], output[\"hidden_states\"][layer][: ,-1 , :].cpu()), dim=0)\n",
    "\n",
    "        \n",
    "        queries_batch = []\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222a97a0",
   "metadata": {
    "libroFormatter": "formatter-string",
    "tags": []
   },
   "source": [
    "# **PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e739eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "333ff73d",
   "metadata": {
    "libroFormatter": "formatter-string",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 600 samples from GSM Symbolic\n",
      "Loaded 164 samples from HumanEval\n",
      "#####Running on ceval_liberal test set\n",
      "The size is 746\n",
      "layers_to_cache_other:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "746it [00:14, 49.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Running on gsm8k test set\n",
      "The size is 1319\n",
      "layers_to_cache_other:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1319it [00:33, 39.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Running on mgsm test set\n",
      "The size is 1250\n",
      "layers_to_cache_other:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1250it [01:12, 17.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Running on popqa test set\n",
      "The size is 14267\n",
      "layers_to_cache_other:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14267it [13:36, 17.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Running on gsm-symbolic test set\n",
      "The size is 600\n",
      "layers_to_cache_other:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [00:05, 105.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Running on human_eval test set\n",
      "The size is 164\n",
      "layers_to_cache_other:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [00:01, 110.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Running on human_eval test set\n",
      "The size is 164\n",
      "layers_to_cache_other:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [00:01, 101.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####Running on mbpp test set\n",
      "The size is 500\n",
      "layers_to_cache_other:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "500it [00:06, 81.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# loading and running gsm8k or other dataset\n",
    "\n",
    "\n",
    "gsm8k_ds_main = load_from_disk('../dataset/gsm8k/main') \n",
    "gsm8k_ds_main_test = list(gsm8k_ds_main['test'])\n",
    "\n",
    "mbpp_ds_full = load_from_disk('../dataset/mbpp/full')\n",
    "mbpp_ds_full_val = list(mbpp_ds_full['validation'])\n",
    "\n",
    "mbpp_ds_full = load_from_disk('../dataset/mbpp/full')\n",
    "mbpp_ds_full_test = list(mbpp_ds_full['test'])\n",
    "\n",
    "# example on MGSM, feel free to add other categories in MGSM\n",
    "mgsm_zh = pd.read_csv('../dataset/mgsm/mgsm_zh.tsv', sep='\\t', header=None)\n",
    "mgsm_zh_test = mgsm_zh.values.tolist()\n",
    "mgsm_de = pd.read_csv('../dataset/mgsm/mgsm_de.tsv', sep='\\t', header=None)\n",
    "mgsm_de_test = mgsm_de.values.tolist()\n",
    "mgsm_bn = pd.read_csv('../dataset/mgsm/mgsm_bn.tsv', sep='\\t', header=None)\n",
    "mgsm_bn_test = mgsm_bn.values.tolist()\n",
    "mgsm_ja = pd.read_csv('../dataset/mgsm/mgsm_ja.tsv', sep='\\t', header=None)\n",
    "mgsm_ja_test = mgsm_ja.values.tolist()\n",
    "mgsm_te = pd.read_csv('../dataset/mgsm/mgsm_te.tsv', sep='\\t', header=None)\n",
    "mgsm_te_test = mgsm_te.values.tolist()\n",
    "\n",
    "\n",
    "# example on C-Eval, feel free to add other categories in C-Eval\n",
    "ceval_chi = pd.read_csv('../dataset/ceval-exam/test/chinese_language_and_literature_test.csv')['question']\n",
    "ceval_chi_test = ceval_chi.tolist()\n",
    "ceval_his = pd.read_csv('../dataset/ceval-exam/test/high_school_history_test.csv')['question']\n",
    "ceval_his_test = ceval_his.tolist()\n",
    "ceval_pol = pd.read_csv('../dataset/ceval-exam/test/high_school_politics_test.csv')['question']\n",
    "ceval_pol_test = ceval_pol.tolist()\n",
    "ceval_mar = pd.read_csv('../dataset/ceval-exam/test/marxism_test.csv')['question']\n",
    "ceval_mar_test = ceval_mar.tolist()\n",
    "ceval_bus = pd.read_csv('../dataset/ceval-exam/test/business_administration_test.csv')['question']\n",
    "ceval_bus_test = ceval_bus.tolist()\n",
    "\n",
    "popqa_test = load_from_disk('../dataset/PopQA/test') \n",
    "popqa_test = list(popqa_test)\n",
    "\n",
    "\n",
    "def load_jsonl_file(jsonl_path, max_samples=None):\n",
    "    \"\"\"Loads .jsonl files line by line.\"\"\"\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    data = [json.loads(line) for line in lines]\n",
    "    return data if max_samples is None else data[:max_samples]\n",
    "\n",
    "def load_gsm_symbolic_data(base_dir, max_samples_per_file=200):\n",
    "    \"\"\"Loads multiple JSONL files from gsm_symbolic_data.\"\"\"\n",
    "    combined = []\n",
    "    for fname in [\"GSM_p1.jsonl\", \"GSM_p2.jsonl\", \"GSM_symbolic.jsonl\"]:\n",
    "        path = os.path.join(base_dir, fname)\n",
    "        if os.path.exists(path):\n",
    "            combined.extend(load_jsonl_file(path, max_samples_per_file))\n",
    "    return combined\n",
    "\n",
    "# --- Paths (adjust if needed) ---\n",
    "DATASET_ROOT = \"../dataset\"\n",
    "\n",
    "gsm_sym_data = load_gsm_symbolic_data(os.path.join(DATASET_ROOT, \"gsm_symbolic_data\"))\n",
    "human_eval_path = os.path.join(DATASET_ROOT, \"HumanEval\", \"HumanEval.jsonl\")\n",
    "human_eval_data = load_jsonl_file(human_eval_path, max_samples=600)\n",
    "\n",
    "print(f\"Loaded {len(gsm_sym_data)} samples from GSM Symbolic\")\n",
    "print(f\"Loaded {len(human_eval_data)} samples from HumanEval\")\n",
    "\n",
    "\n",
    "other_running_set_name_list = ['ceval_liberal', 'gsm8k', 'mgsm', 'popqa', 'gsm_symbolic', 'human_eval', 'human_eval', 'mbpp'] # mbpp,, hoppingtoolate， , 'mbpp', 'popqa',\n",
    "# other_running_set_name_list = [ 'gsm_symbolic', 'human_eval', 'human_eval', 'mbpp']\n",
    "# other_running_set_name_list = ['ceval_liberal', 'gsm8k', 'mgsm', 'popqa'] # mbpp,, hoppingtoolate， , 'mbpp', 'popqa',\n",
    "\n",
    "\n",
    "other_dataset = None\n",
    "\n",
    "hs_cache_no_cot_other_all = {}\n",
    "\n",
    "for other_running_set_name in other_running_set_name_list:\n",
    "    \n",
    "    if other_running_set_name == 'mbpp':\n",
    "        other_dataset = mbpp_ds_full_test\n",
    "         \n",
    "    elif other_running_set_name == 'gsm8k':\n",
    "        other_dataset = gsm8k_ds_main_test\n",
    "         \n",
    "    elif other_running_set_name == 'mgsm': #multilingual gsm8k\n",
    "        other_dataset = mgsm_zh_test + mgsm_de_test + mgsm_bn_test + mgsm_ja_test + mgsm_te_test\n",
    "         \n",
    "    elif other_running_set_name == 'ceval_liberal':\n",
    "        other_dataset = ceval_chi_test + ceval_his_test + ceval_pol_test + ceval_mar_test # + ceval_bus_test\n",
    "         \n",
    "    elif other_running_set_name == 'popqa': #multilingual gsm8k\n",
    "        other_dataset = popqa_test\n",
    "         \n",
    "    elif other_running_set_name == 'gsm_symbolic':\n",
    "        other_dataset = gsm_sym_data\n",
    "         \n",
    "    elif other_running_set_name == 'human_eval':\n",
    "        other_dataset = human_eval_data\n",
    "         \n",
    "    elif other_running_set_name == 'gsm_symbolic':\n",
    "        other_dataset = gsm_sym_data\n",
    "         \n",
    "\n",
    "\n",
    "    print(f'#####Running on {other_running_set_name} test set')\n",
    "    print(f'The size is {len(other_dataset)}')\n",
    "\n",
    "    layers_to_cache_other = list(range(model_layers_num))\n",
    "    print('layers_to_cache_other: ',layers_to_cache_other)\n",
    "    hs_cache_no_cot_other = {}\n",
    "    queries_batch = []\n",
    "    batch_size = 4\n",
    "\n",
    "    for ix, entry in tqdm(enumerate(other_dataset)):\n",
    "\n",
    "        if other_running_set_name == 'gsm8k':\n",
    "            query = 'Q: ' + entry['question'] + \"\\nA: \"\n",
    "        elif other_running_set_name == 'mbpp':\n",
    "            query = 'Q: ' + entry['text'] + \"\\nA: \"\n",
    "        elif other_running_set_name == 'mgsm':\n",
    "            query = 'Q: ' + entry[0] + \"\\nA: \"\n",
    "        elif other_running_set_name == 'ceval_liberal':\n",
    "            query = 'Q: ' + entry + \"\\nA: \"\n",
    "        elif other_running_set_name == 'popqa':\n",
    "            query = 'Q: ' + entry['question'] + \"\\nA: \"\n",
    "\n",
    "        queries_batch.append(query)\n",
    "\n",
    "        if len(queries_batch) == batch_size or ix == len(other_dataset) - 1:\n",
    "            output = generate_outputs(queries_batch)\n",
    "\n",
    "            for layer in layers_to_cache_other:\n",
    "                if layer not in hs_cache_no_cot_other:\n",
    "                    hs_cache_no_cot_other[layer] = output[\"hidden_states\"][layer][: ,-1 , :].cpu() #bs * tok * dims\n",
    "                else:\n",
    "                    hs_cache_no_cot_other[layer] = torch.cat((hs_cache_no_cot_other[layer], output[\"hidden_states\"][layer][: ,-1 , :].cpu()), dim=0)\n",
    "\n",
    "            queries_batch = []\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    hs_cache_no_cot_other_all[other_running_set_name] = hs_cache_no_cot_other\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85bcd615",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_cache_no_cot_other_all['mmlu-pro_600'] = hs_cache_no_cot\n",
    "hs_cache_no_cot_other_all['mmlu-pro_3000'] = hs_cache_no_cot_3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b59295ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subj</th>\n",
       "      <th>prop</th>\n",
       "      <th>obj</th>\n",
       "      <th>subj_id</th>\n",
       "      <th>prop_id</th>\n",
       "      <th>obj_id</th>\n",
       "      <th>s_aliases</th>\n",
       "      <th>o_aliases</th>\n",
       "      <th>s_uri</th>\n",
       "      <th>o_uri</th>\n",
       "      <th>s_wiki_title</th>\n",
       "      <th>o_wiki_title</th>\n",
       "      <th>s_pop</th>\n",
       "      <th>o_pop</th>\n",
       "      <th>question</th>\n",
       "      <th>possible_answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4222362</td>\n",
       "      <td>George Rankin</td>\n",
       "      <td>occupation</td>\n",
       "      <td>politician</td>\n",
       "      <td>1850297</td>\n",
       "      <td>22</td>\n",
       "      <td>2834605</td>\n",
       "      <td>[\"George James Rankin\"]</td>\n",
       "      <td>[\"political leader\",\"political figure\",\"polit....</td>\n",
       "      <td>http://www.wikidata.org/entity/Q5543720</td>\n",
       "      <td>http://www.wikidata.org/entity/Q82955</td>\n",
       "      <td>George Rankin</td>\n",
       "      <td>Politician</td>\n",
       "      <td>142</td>\n",
       "      <td>25692</td>\n",
       "      <td>What is George Rankin's occupation?</td>\n",
       "      <td>[\"politician\", \"political leader\", \"political ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4725190</td>\n",
       "      <td>John Mayne</td>\n",
       "      <td>occupation</td>\n",
       "      <td>journalist</td>\n",
       "      <td>2079053</td>\n",
       "      <td>22</td>\n",
       "      <td>663400</td>\n",
       "      <td>[]</td>\n",
       "      <td>[\"journo\",\"journalists\"]</td>\n",
       "      <td>http://www.wikidata.org/entity/Q6247345</td>\n",
       "      <td>http://www.wikidata.org/entity/Q1930187</td>\n",
       "      <td>John Mayne</td>\n",
       "      <td>Journalist</td>\n",
       "      <td>236</td>\n",
       "      <td>24952</td>\n",
       "      <td>What is John Mayne's occupation?</td>\n",
       "      <td>[\"journalist\", \"journo\", \"journalists\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4382392</td>\n",
       "      <td>Henry Feilden</td>\n",
       "      <td>occupation</td>\n",
       "      <td>politician</td>\n",
       "      <td>1925450</td>\n",
       "      <td>22</td>\n",
       "      <td>2834605</td>\n",
       "      <td>[\"Henry Master Feilden\"]</td>\n",
       "      <td>[\"political leader\",\"political figure\",\"polit....</td>\n",
       "      <td>http://www.wikidata.org/entity/Q5725578</td>\n",
       "      <td>http://www.wikidata.org/entity/Q82955</td>\n",
       "      <td>Henry Feilden (Conservative politician)</td>\n",
       "      <td>Politician</td>\n",
       "      <td>58</td>\n",
       "      <td>25692</td>\n",
       "      <td>What is Henry Feilden's occupation?</td>\n",
       "      <td>[\"politician\", \"political leader\", \"political ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4822110</td>\n",
       "      <td>Kathy Saltzman</td>\n",
       "      <td>occupation</td>\n",
       "      <td>politician</td>\n",
       "      <td>2122743</td>\n",
       "      <td>22</td>\n",
       "      <td>2834605</td>\n",
       "      <td>[]</td>\n",
       "      <td>[\"political leader\",\"political figure\",\"polit....</td>\n",
       "      <td>http://www.wikidata.org/entity/Q6377295</td>\n",
       "      <td>http://www.wikidata.org/entity/Q82955</td>\n",
       "      <td>Kathy Saltzman</td>\n",
       "      <td>Politician</td>\n",
       "      <td>127</td>\n",
       "      <td>25692</td>\n",
       "      <td>What is Kathy Saltzman's occupation?</td>\n",
       "      <td>[\"politician\", \"political leader\", \"political ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4011112</td>\n",
       "      <td>Eleanor Davis</td>\n",
       "      <td>occupation</td>\n",
       "      <td>cartoonist</td>\n",
       "      <td>1752619</td>\n",
       "      <td>22</td>\n",
       "      <td>68412</td>\n",
       "      <td>[\"Eleanor McCutcheon Davis\"]</td>\n",
       "      <td>[\"graphic artist\",\"animator\",\"illustrator\"]</td>\n",
       "      <td>http://www.wikidata.org/entity/Q5354261</td>\n",
       "      <td>http://www.wikidata.org/entity/Q1114448</td>\n",
       "      <td>Eleanor Davis</td>\n",
       "      <td>Cartoonist</td>\n",
       "      <td>317</td>\n",
       "      <td>9649</td>\n",
       "      <td>What is Eleanor Davis's occupation?</td>\n",
       "      <td>[\"cartoonist\", \"graphic artist\", \"animator\", \"...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14262</th>\n",
       "      <td>4165948</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>capital</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>1825504</td>\n",
       "      <td>422</td>\n",
       "      <td>2455815</td>\n",
       "      <td>[\"Holland\",\"the Netherlands\",\"NL\",\"NED\",\"Neder...</td>\n",
       "      <td>[\"Mokum\",\"Amsterdam, NL\",\"Amsterdam, Netherlan...</td>\n",
       "      <td>http://www.wikidata.org/entity/Q55</td>\n",
       "      <td>http://www.wikidata.org/entity/Q727</td>\n",
       "      <td>Netherlands</td>\n",
       "      <td>Amsterdam</td>\n",
       "      <td>416350</td>\n",
       "      <td>147710</td>\n",
       "      <td>What is the capital of Netherlands?</td>\n",
       "      <td>[\"The Hague\", \"Den Haag\", \"'s-Gravenhage\", \"Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14263</th>\n",
       "      <td>2617063</td>\n",
       "      <td>City Municipality of Celje</td>\n",
       "      <td>capital</td>\n",
       "      <td>Celje</td>\n",
       "      <td>1127153</td>\n",
       "      <td>422</td>\n",
       "      <td>5035</td>\n",
       "      <td>[\"Celje city municipality\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>http://www.wikidata.org/entity/Q3441823</td>\n",
       "      <td>http://www.wikidata.org/entity/Q1012</td>\n",
       "      <td>City Municipality of Celje</td>\n",
       "      <td>Celje</td>\n",
       "      <td>335</td>\n",
       "      <td>4086</td>\n",
       "      <td>What is the capital of City Municipality of Ce...</td>\n",
       "      <td>[\"Celje\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14264</th>\n",
       "      <td>471864</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>capital</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>192624</td>\n",
       "      <td>422</td>\n",
       "      <td>2938512</td>\n",
       "      <td>[\"National Capital Region of Delhi\",\"NCR\"]</td>\n",
       "      <td>[\"New Delhi district\",\"Nayi Dilli\"]</td>\n",
       "      <td>http://www.wikidata.org/entity/Q1353</td>\n",
       "      <td>http://www.wikidata.org/entity/Q987</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>219227</td>\n",
       "      <td>128193</td>\n",
       "      <td>What is the capital of Delhi?</td>\n",
       "      <td>[\"New Delhi\", \"New Delhi district\", \"Nayi Dilli\"]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14265</th>\n",
       "      <td>2965573</td>\n",
       "      <td>Palestinian territories</td>\n",
       "      <td>capital</td>\n",
       "      <td>Jerusalem</td>\n",
       "      <td>1270077</td>\n",
       "      <td>422</td>\n",
       "      <td>127625</td>\n",
       "      <td>[\"occupied Palestinian territories\",\"OPT\",\"Wes...</td>\n",
       "      <td>[\"Yerushalayim\",\"J'lem\",\"Aelia Capitolina\",\"Al...</td>\n",
       "      <td>http://www.wikidata.org/entity/Q407199</td>\n",
       "      <td>http://www.wikidata.org/entity/Q1218</td>\n",
       "      <td>Palestinian territories</td>\n",
       "      <td>Jerusalem</td>\n",
       "      <td>17545</td>\n",
       "      <td>136734</td>\n",
       "      <td>What is the capital of Palestinian territories?</td>\n",
       "      <td>[\"Jerusalem\", \"Yerushalayim\", \"J'lem\", \"Aelia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14266</th>\n",
       "      <td>802449</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>capital</td>\n",
       "      <td>Baton Rouge</td>\n",
       "      <td>342549</td>\n",
       "      <td>422</td>\n",
       "      <td>940158</td>\n",
       "      <td>[\"LA\",\"State of Louisiana\",\"18th State\",\"Louis...</td>\n",
       "      <td>[\"Baton Rouge, Louisiana\",\"Baton Rouge, LA\",\"B...</td>\n",
       "      <td>http://www.wikidata.org/entity/Q1588</td>\n",
       "      <td>http://www.wikidata.org/entity/Q28218</td>\n",
       "      <td>Louisiana</td>\n",
       "      <td>Baton Rouge, Louisiana</td>\n",
       "      <td>242631</td>\n",
       "      <td>39365</td>\n",
       "      <td>What is the capital of Louisiana?</td>\n",
       "      <td>[\"Baton Rouge\", \"Baton Rouge, Louisiana\", \"Bat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14267 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                        subj        prop          obj  subj_id  \\\n",
       "0      4222362               George Rankin  occupation   politician  1850297   \n",
       "1      4725190                  John Mayne  occupation   journalist  2079053   \n",
       "2      4382392               Henry Feilden  occupation   politician  1925450   \n",
       "3      4822110              Kathy Saltzman  occupation   politician  2122743   \n",
       "4      4011112               Eleanor Davis  occupation   cartoonist  1752619   \n",
       "...        ...                         ...         ...          ...      ...   \n",
       "14262  4165948                 Netherlands     capital    Amsterdam  1825504   \n",
       "14263  2617063  City Municipality of Celje     capital        Celje  1127153   \n",
       "14264   471864                       Delhi     capital    New Delhi   192624   \n",
       "14265  2965573     Palestinian territories     capital    Jerusalem  1270077   \n",
       "14266   802449                   Louisiana     capital  Baton Rouge   342549   \n",
       "\n",
       "       prop_id   obj_id                                          s_aliases  \\\n",
       "0           22  2834605                            [\"George James Rankin\"]   \n",
       "1           22   663400                                                 []   \n",
       "2           22  2834605                           [\"Henry Master Feilden\"]   \n",
       "3           22  2834605                                                 []   \n",
       "4           22    68412                       [\"Eleanor McCutcheon Davis\"]   \n",
       "...        ...      ...                                                ...   \n",
       "14262      422  2455815  [\"Holland\",\"the Netherlands\",\"NL\",\"NED\",\"Neder...   \n",
       "14263      422     5035                        [\"Celje city municipality\"]   \n",
       "14264      422  2938512         [\"National Capital Region of Delhi\",\"NCR\"]   \n",
       "14265      422   127625  [\"occupied Palestinian territories\",\"OPT\",\"Wes...   \n",
       "14266      422   940158  [\"LA\",\"State of Louisiana\",\"18th State\",\"Louis...   \n",
       "\n",
       "                                               o_aliases  \\\n",
       "0      [\"political leader\",\"political figure\",\"polit....   \n",
       "1                               [\"journo\",\"journalists\"]   \n",
       "2      [\"political leader\",\"political figure\",\"polit....   \n",
       "3      [\"political leader\",\"political figure\",\"polit....   \n",
       "4            [\"graphic artist\",\"animator\",\"illustrator\"]   \n",
       "...                                                  ...   \n",
       "14262  [\"Mokum\",\"Amsterdam, NL\",\"Amsterdam, Netherlan...   \n",
       "14263                                                 []   \n",
       "14264                [\"New Delhi district\",\"Nayi Dilli\"]   \n",
       "14265  [\"Yerushalayim\",\"J'lem\",\"Aelia Capitolina\",\"Al...   \n",
       "14266  [\"Baton Rouge, Louisiana\",\"Baton Rouge, LA\",\"B...   \n",
       "\n",
       "                                         s_uri  \\\n",
       "0      http://www.wikidata.org/entity/Q5543720   \n",
       "1      http://www.wikidata.org/entity/Q6247345   \n",
       "2      http://www.wikidata.org/entity/Q5725578   \n",
       "3      http://www.wikidata.org/entity/Q6377295   \n",
       "4      http://www.wikidata.org/entity/Q5354261   \n",
       "...                                        ...   \n",
       "14262       http://www.wikidata.org/entity/Q55   \n",
       "14263  http://www.wikidata.org/entity/Q3441823   \n",
       "14264     http://www.wikidata.org/entity/Q1353   \n",
       "14265   http://www.wikidata.org/entity/Q407199   \n",
       "14266     http://www.wikidata.org/entity/Q1588   \n",
       "\n",
       "                                         o_uri  \\\n",
       "0        http://www.wikidata.org/entity/Q82955   \n",
       "1      http://www.wikidata.org/entity/Q1930187   \n",
       "2        http://www.wikidata.org/entity/Q82955   \n",
       "3        http://www.wikidata.org/entity/Q82955   \n",
       "4      http://www.wikidata.org/entity/Q1114448   \n",
       "...                                        ...   \n",
       "14262      http://www.wikidata.org/entity/Q727   \n",
       "14263     http://www.wikidata.org/entity/Q1012   \n",
       "14264      http://www.wikidata.org/entity/Q987   \n",
       "14265     http://www.wikidata.org/entity/Q1218   \n",
       "14266    http://www.wikidata.org/entity/Q28218   \n",
       "\n",
       "                                  s_wiki_title            o_wiki_title  \\\n",
       "0                                George Rankin              Politician   \n",
       "1                                   John Mayne              Journalist   \n",
       "2      Henry Feilden (Conservative politician)              Politician   \n",
       "3                               Kathy Saltzman              Politician   \n",
       "4                                Eleanor Davis              Cartoonist   \n",
       "...                                        ...                     ...   \n",
       "14262                              Netherlands               Amsterdam   \n",
       "14263               City Municipality of Celje                   Celje   \n",
       "14264                                    Delhi               New Delhi   \n",
       "14265                  Palestinian territories               Jerusalem   \n",
       "14266                                Louisiana  Baton Rouge, Louisiana   \n",
       "\n",
       "        s_pop   o_pop                                           question  \\\n",
       "0         142   25692                What is George Rankin's occupation?   \n",
       "1         236   24952                   What is John Mayne's occupation?   \n",
       "2          58   25692                What is Henry Feilden's occupation?   \n",
       "3         127   25692               What is Kathy Saltzman's occupation?   \n",
       "4         317    9649                What is Eleanor Davis's occupation?   \n",
       "...       ...     ...                                                ...   \n",
       "14262  416350  147710                What is the capital of Netherlands?   \n",
       "14263     335    4086  What is the capital of City Municipality of Ce...   \n",
       "14264  219227  128193                      What is the capital of Delhi?   \n",
       "14265   17545  136734    What is the capital of Palestinian territories?   \n",
       "14266  242631   39365                  What is the capital of Louisiana?   \n",
       "\n",
       "                                        possible_answers  \n",
       "0      [\"politician\", \"political leader\", \"political ...  \n",
       "1                [\"journalist\", \"journo\", \"journalists\"]  \n",
       "2      [\"politician\", \"political leader\", \"political ...  \n",
       "3      [\"politician\", \"political leader\", \"political ...  \n",
       "4      [\"cartoonist\", \"graphic artist\", \"animator\", \"...  \n",
       "...                                                  ...  \n",
       "14262  [\"The Hague\", \"Den Haag\", \"'s-Gravenhage\", \"Ha...  \n",
       "14263                                          [\"Celje\"]  \n",
       "14264  [\"New Delhi\", \"New Delhi district\", \"Nayi Dilli\"]  \n",
       "14265  [\"Jerusalem\", \"Yerushalayim\", \"J'lem\", \"Aelia ...  \n",
       "14266  [\"Baton Rouge\", \"Baton Rouge, Louisiana\", \"Bat...  \n",
       "\n",
       "[14267 rows x 17 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(other_dataset)#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca94154",
   "metadata": {},
   "source": [
    "# Let's take a look at the output vector shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da2dcb0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ceval_liberal shape: torch.Size([746, 1536])\n",
      "gsm8k shape: torch.Size([1319, 1536])\n",
      "mgsm shape: torch.Size([1250, 1536])\n",
      "popqa shape: torch.Size([14267, 1536])\n",
      "gsm-symbolic shape: torch.Size([600, 1536])\n",
      "human_eval shape: torch.Size([164, 1536])\n",
      "mbpp shape: torch.Size([500, 1536])\n",
      "mmlu-pro_600 shape: torch.Size([600, 1536])\n",
      "mmlu-pro_3000 shape: torch.Size([3000, 1536])\n"
     ]
    }
   ],
   "source": [
    "for k, v in hs_cache_no_cot_other_all.items():\n",
    "    print(f\"{k} shape: {v[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee29dd0d",
   "metadata": {},
   "source": [
    "# Let's take a look at the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0aff2de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>instance</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>original_id</th>\n",
       "      <th>original_question</th>\n",
       "      <th>original_answer</th>\n",
       "      <th>canary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sanjay saw a 60-foot dolphin with 16 12-inch r...</td>\n",
       "      <td>First, find the total number of remoras remain...</td>\n",
       "      <td>473</td>\n",
       "      <td>Benny saw a 10-foot shark with 2 6-inch remora...</td>\n",
       "      <td>First, find the combined length of the remoras...</td>\n",
       "      <td>This document should not appear in training co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Tara saw a 70-foot dolphin with 14 12-inch rem...</td>\n",
       "      <td>First, find the total number of remoras remain...</td>\n",
       "      <td>473</td>\n",
       "      <td>Benny saw a 10-foot shark with 2 6-inch remora...</td>\n",
       "      <td>First, find the combined length of the remoras...</td>\n",
       "      <td>This document should not appear in training co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Liam saw a 50-foot shark with 12 15-inch remor...</td>\n",
       "      <td>First, find the total number of remoras remain...</td>\n",
       "      <td>473</td>\n",
       "      <td>Benny saw a 10-foot shark with 2 6-inch remora...</td>\n",
       "      <td>First, find the combined length of the remoras...</td>\n",
       "      <td>This document should not appear in training co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Santiago saw a 80-foot dolphin with 12 24-inch...</td>\n",
       "      <td>First, find the total number of remoras remain...</td>\n",
       "      <td>473</td>\n",
       "      <td>Benny saw a 10-foot shark with 2 6-inch remora...</td>\n",
       "      <td>First, find the combined length of the remoras...</td>\n",
       "      <td>This document should not appear in training co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Ananya saw a 70-foot dolphin with 14 12-inch r...</td>\n",
       "      <td>First, find the total number of remoras remain...</td>\n",
       "      <td>473</td>\n",
       "      <td>Benny saw a 10-foot shark with 2 6-inch remora...</td>\n",
       "      <td>First, find the combined length of the remoras...</td>\n",
       "      <td>This document should not appear in training co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  instance                                           question  \\\n",
       "0   0         0  Sanjay saw a 60-foot dolphin with 16 12-inch r...   \n",
       "1   0         1  Tara saw a 70-foot dolphin with 14 12-inch rem...   \n",
       "2   0         2  Liam saw a 50-foot shark with 12 15-inch remor...   \n",
       "3   0         3  Santiago saw a 80-foot dolphin with 12 24-inch...   \n",
       "4   0         4  Ananya saw a 70-foot dolphin with 14 12-inch r...   \n",
       "\n",
       "                                              answer  original_id  \\\n",
       "0  First, find the total number of remoras remain...          473   \n",
       "1  First, find the total number of remoras remain...          473   \n",
       "2  First, find the total number of remoras remain...          473   \n",
       "3  First, find the total number of remoras remain...          473   \n",
       "4  First, find the total number of remoras remain...          473   \n",
       "\n",
       "                                   original_question  \\\n",
       "0  Benny saw a 10-foot shark with 2 6-inch remora...   \n",
       "1  Benny saw a 10-foot shark with 2 6-inch remora...   \n",
       "2  Benny saw a 10-foot shark with 2 6-inch remora...   \n",
       "3  Benny saw a 10-foot shark with 2 6-inch remora...   \n",
       "4  Benny saw a 10-foot shark with 2 6-inch remora...   \n",
       "\n",
       "                                     original_answer  \\\n",
       "0  First, find the combined length of the remoras...   \n",
       "1  First, find the combined length of the remoras...   \n",
       "2  First, find the combined length of the remoras...   \n",
       "3  First, find the combined length of the remoras...   \n",
       "4  First, find the combined length of the remoras...   \n",
       "\n",
       "                                              canary  \n",
       "0  This document should not appear in training co...  \n",
       "1  This document should not appear in training co...  \n",
       "2  This document should not appear in training co...  \n",
       "3  This document should not appear in training co...  \n",
       "4  This document should not appear in training co...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.DataFrame(gsm8k_ds_main['train']).head()\n",
    "pd.DataFrame(gsm_sym_data).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f5634b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>text</th>\n",
       "      <th>code</th>\n",
       "      <th>test_list</th>\n",
       "      <th>test_setup_code</th>\n",
       "      <th>challenge_test_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>Write a python function to remove first and la...</td>\n",
       "      <td>def remove_Occ(s,ch): \\r\\n    for i in range(l...</td>\n",
       "      <td>[assert remove_Occ(\"hello\",\"l\") == \"heo\", asse...</td>\n",
       "      <td></td>\n",
       "      <td>[assert remove_Occ(\"hellolloll\",\"l\") == \"helol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>Write a function to sort a given matrix in asc...</td>\n",
       "      <td>def sort_matrix(M):\\r\\n    result = sorted(M, ...</td>\n",
       "      <td>[assert sort_matrix([[1, 2, 3], [2, 4, 5], [1,...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>Write a function to count the most common word...</td>\n",
       "      <td>from collections import Counter\\r\\ndef count_c...</td>\n",
       "      <td>[assert count_common(['red','green','black','p...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>Write a python function to find the volume of ...</td>\n",
       "      <td>def find_Volume(l,b,h) : \\r\\n    return ((l * ...</td>\n",
       "      <td>[assert find_Volume(10,8,6) == 240, assert fin...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>Write a function to split a string at lowercas...</td>\n",
       "      <td>import re\\r\\ndef split_lowerstring(text):\\r\\n ...</td>\n",
       "      <td>[assert split_lowerstring(\"AbCd\")==['bC','d'],...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   task_id                                               text  \\\n",
       "0       11  Write a python function to remove first and la...   \n",
       "1       12  Write a function to sort a given matrix in asc...   \n",
       "2       13  Write a function to count the most common word...   \n",
       "3       14  Write a python function to find the volume of ...   \n",
       "4       15  Write a function to split a string at lowercas...   \n",
       "\n",
       "                                                code  \\\n",
       "0  def remove_Occ(s,ch): \\r\\n    for i in range(l...   \n",
       "1  def sort_matrix(M):\\r\\n    result = sorted(M, ...   \n",
       "2  from collections import Counter\\r\\ndef count_c...   \n",
       "3  def find_Volume(l,b,h) : \\r\\n    return ((l * ...   \n",
       "4  import re\\r\\ndef split_lowerstring(text):\\r\\n ...   \n",
       "\n",
       "                                           test_list test_setup_code  \\\n",
       "0  [assert remove_Occ(\"hello\",\"l\") == \"heo\", asse...                   \n",
       "1  [assert sort_matrix([[1, 2, 3], [2, 4, 5], [1,...                   \n",
       "2  [assert count_common(['red','green','black','p...                   \n",
       "3  [assert find_Volume(10,8,6) == 240, assert fin...                   \n",
       "4  [assert split_lowerstring(\"AbCd\")==['bC','d'],...                   \n",
       "\n",
       "                                 challenge_test_list  \n",
       "0  [assert remove_Occ(\"hellolloll\",\"l\") == \"helol...  \n",
       "1                                                 []  \n",
       "2                                                 []  \n",
       "3                                                 []  \n",
       "4                                                 []  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(other_dataset).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb359c4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ceval_liberal',\n",
       " 'gsm8k',\n",
       " 'mgsm',\n",
       " 'popqa',\n",
       " 'gsm-symbolic',\n",
       " 'human_eval',\n",
       " 'mbpp',\n",
       " 'mmlu-pro_600',\n",
       " 'mmlu-pro_3000']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(hs_cache_no_cot_other_all.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2500eb",
   "metadata": {},
   "source": [
    "# save vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65ae6022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ceval_liberal', 'gsm8k', 'mgsm', 'popqa', 'mmlu-pro_3000', 'mmlu-pro_600', 'human_eval', 'mbpp', 'gsm_symbolic'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs_cache_no_cot_other_all.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d31c9b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "hs_cache_no_cot_other_all['gsm_symbolic'] = hs_cache_no_cot_other_all['gsm-symbolic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b3d7db6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_path = '../reasoning_representations_outputs'\n",
    "os.makedirs(save_path, exist_ok=True)  \n",
    "\n",
    "torch.save(hs_cache_no_cot_other_all, os.path.join(save_path, f'{model_name}-base_hs_cache_no_cot_all.pt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python py311",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
